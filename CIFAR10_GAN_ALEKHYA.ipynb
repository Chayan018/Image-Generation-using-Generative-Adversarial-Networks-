{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "filter = np.where((train_labels ==6) | (train_labels == 7) | (train_labels == 8))\n",
        "train_images = train_images[filter[0]].astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5\n"
      ],
      "metadata": {
        "id": "qPf_PKSZPLnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (3,3), padding='same', input_shape=(32,32,3)),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2D(128, (3,3), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2D(128, (3,3), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Added extra layers for more capacity\n",
        "        layers.Conv2D(256, (3,3), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2D(512, (3,3), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Conv2D(512, (3,3), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "y9HfGCEhPNg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zgw3teIg45Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        # Input projection and reshape\n",
        "        layers.Dense(512*4*4, input_dim=100),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((4,4,512)),\n",
        "\n",
        "        # First upsampling block (4x4 → 8x8)\n",
        "        layers.Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Additional conv layer at same resolution\n",
        "        layers.Conv2D(256, (3,3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Second upsampling block (8x8 → 16x16)\n",
        "        layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Additional conv layer at same resolution\n",
        "        layers.Conv2D(128, (3,3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Third upsampling block (16x16 → 32x32)\n",
        "        layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Additional refinement layers at final resolution\n",
        "        layers.Conv2D(64, (3,3), padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "        # Final output layer\n",
        "        layers.Conv2D(3, (3,3), activation='tanh', padding='same'),\n",
        "    ])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "61ki6T28PPIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "        disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + \\\n",
        "                    cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_gen, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def generate_images(epoch):\n",
        "    predictions = generator(tf.random.normal([16, 100]), training=False)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow((predictions[i] * 127.5 + 127.5).numpy().astype('uint8'))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "mcdeq-PfPR0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images) \\\n",
        "    .shuffle(6000).batch(BATCH_SIZE)\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for image_batch in train_dataset:\n",
        "        gen_loss, disc_loss = train_step(image_batch)\n",
        "\n",
        "    generate_images(epoch)\n",
        "    print(f'Epoch {epoch+1} | Gen loss: {gen_loss:.4f} | Disc loss: {disc_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "Q04lBH1jPSic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(900):\n",
        "    for image_batch in train_dataset:\n",
        "        gen_loss, disc_loss = train_step(image_batch)\n",
        "\n",
        "    generate_images(epoch)\n",
        "    print(f'Epoch {epoch+1} | Gen loss: {gen_loss:.4f} | Disc loss: {disc_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "-p4FeADkFA4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save('generator_cifar10_1000epochs.keras')\n",
        "discriminator.save('discriminator_cifar10_1000epochs.keras')"
      ],
      "metadata": {
        "id": "oSzAAOdiWRLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}